# coding:utf-8

import glob
import hashlib

# 刪除重複的文件
#data = {'name': {'path/name': 'content', 'path2/name': 'content'}}
data = {}

def clear(path):
    result = glob.glob(path)

    for _data in result:
        if glob.os.path.isdir(_data):
            _path = glob.os.path.join(_data, '*')
            clear(_path)
        else:
            name = glob.os.path.split(_data)[-1]  # 為了避開'zip'的文件名

            is_byte = False

            if 'zip' in name:
                is_byte = True
                f = open(_data, 'rb')
            else:
                f = open(_data, 'r', encoding='utf-8')
            content = f.read()
            f.close()

            if is_byte:
                hash_content_obj = hashlib.md5(content)
            else:
                hash_content_obj = hashlib.md5(content.encode('utf-8'))
            hash_content = hash_content_obj.hexdigest()

            if name in data:  # 要確保文件名在字典內
                sub_data = data[name]  # # v -‘.py’的內容, k - '.py'  =搜尋 為搜第二層的value

                is_delete = False

                for k, v in sub_data.items():
                    if v == hash_content:
                        glob.os.remove(_data)
                        print('%s will delete' % _data)
                        is_delete = True

                if not is_delete:
                    data[name][_data] = hash_content  #　‘.py’的內容


            else:
                # k -'.py',  v -'.py'的內容
                data[name] = {
                    _data: hash_content
                }

if __name__ == '__main__':
    path = glob.os.path.join(glob.os.getcwd(), '*')
    clear(path)
    for k, v in data.items():
        print(k, v)
        for _k, v in v.items():
            print(_k, v)
